{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinecraftPolicy(\n",
       "  (impala): CustomIMPALA(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=8192, out_features=8, bias=True)\n",
       "  )\n",
       "  (recurrence): CustomTransformer(\n",
       "    (embedding): Linear(in_features=8, out_features=2048, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (policy_head): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from agent import MineRLAgent\n",
    "\n",
    "\n",
    "class CustomIMPALA(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom IMPALA CNN for processing image inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, chans, width):\n",
    "        super(CustomIMPALA, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = input_shape[2]  # Number of input channels (e.g., 3 for RGB)\n",
    "\n",
    "        for out_channels in chans:\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2)\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.fc = nn.Linear(chans[-1] * (input_shape[0] // (2 ** len(chans))) * (input_shape[1] // (2 ** len(chans))), width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten for FC layer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for recurrence and attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidsize, num_heads, num_layers, memory_size):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidsize)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, memory_size, hidsize))\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidsize,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=4 * hidsize,\n",
    "                activation=\"relu\"\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MinecraftPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom PyTorch model to replicate the `.model` file.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(MinecraftPolicy, self).__init__()\n",
    "        img_shape = args[\"img_shape\"]\n",
    "        impala_chans = args[\"impala_chans\"]\n",
    "        impala_width = args[\"impala_width\"]\n",
    "        hidsize = args[\"hidsize\"]\n",
    "        attention_heads = args[\"attention_heads\"]\n",
    "        attention_memory_size = args[\"attention_memory_size\"]\n",
    "        n_recurrence_layers = args[\"n_recurrence_layers\"]\n",
    "\n",
    "        # IMPALA CNN for image processing\n",
    "        self.impala = CustomIMPALA(img_shape, impala_chans, impala_width)\n",
    "\n",
    "        # Transformer-based recurrent layers\n",
    "        self.recurrence = CustomTransformer(\n",
    "            input_dim=impala_width,\n",
    "            hidsize=hidsize,\n",
    "            num_heads=attention_heads,\n",
    "            num_layers=n_recurrence_layers,\n",
    "            memory_size=attention_memory_size\n",
    "        )\n",
    "\n",
    "        # Output head\n",
    "        self.policy_head = nn.Linear(hidsize, args.get(\"output_dim\", 10))  # Example output size; adjust as needed\n",
    "\n",
    "    def forward(self, img, timesteps=None):\n",
    "        x = self.impala(img)\n",
    "        if timesteps is not None:\n",
    "            x = x.unsqueeze(1).repeat(1, timesteps, 1)  # Add time dimension\n",
    "        x = self.recurrence(x)\n",
    "        return self.policy_head(x[:, -1, :])  # Use the last timestep\n",
    "\n",
    "\n",
    "# Example instantiation:\n",
    "def create_minecraft_policy(model_args):\n",
    "    return MinecraftPolicy(model_args)\n",
    "\n",
    "\n",
    "# Load model args from the .model file\n",
    "with open(r'F:\\16831_RL\\Proj\\MCRL-Proj\\Model_Weights\\2x_pre\\2x.model', 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "# Extract arguments for the policy\n",
    "model_args = model_data['model']['args']['net']['args']\n",
    "\n",
    "# Create the PyTorch model\n",
    "custom_model = create_minecraft_policy(model_args)\n",
    "\n",
    "# Create a MineRLAgent and attach the custom model\n",
    "agent = MineRLAgent(None, policy_kwargs=model_args)\n",
    "agent.model = custom_model\n",
    "\n",
    "# Load weights into the model\n",
    "weights_path = r'F:\\16831_RL\\Proj\\MCRL-Proj\\Model_Weights\\2x_pre\\rl-from-house-2x.weights'\n",
    "agent.load_weights(weights_path)\n",
    "\n",
    "# Move the model to shared memory for multiprocessing\n",
    "agent.model.share_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test the model with dummy inputs\n",
    "dummy_img = torch.randn(1, *model_args[\"img_shape\"])  # Batch size of 1, shape [1, 128, 128, 3]\n",
    "dummy_img = dummy_img.permute(0, 3, 1, 2)  # Change to [batch_size, channels, height, width]\n",
    "output = custom_model(dummy_img)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(360, 640, 3)\n",
      "Action: {'attack': array([0]), 'back': array([0]), 'forward': array([0]), 'jump': array([0]), 'left': array([0]), 'right': array([0]), 'sneak': array([0]), 'sprint': array([0]), 'use': array([0]), 'drop': array([0]), 'inventory': array([0]), 'hotbar.1': array([0]), 'hotbar.2': array([0]), 'hotbar.3': array([0]), 'hotbar.4': array([0]), 'hotbar.5': array([0]), 'hotbar.6': array([0]), 'hotbar.7': array([0]), 'hotbar.8': array([0]), 'hotbar.9': array([0]), 'camera': array([[-5.80948313, -5.80948313]])}\n",
      "Action: {'attack': array([1]), 'back': array([0]), 'forward': array([0]), 'jump': array([0]), 'left': array([0]), 'right': array([0]), 'sneak': array([0]), 'sprint': array([0]), 'use': array([0]), 'drop': array([0]), 'inventory': array([0]), 'hotbar.1': array([0]), 'hotbar.2': array([0]), 'hotbar.3': array([0]), 'hotbar.4': array([0]), 'hotbar.5': array([0]), 'hotbar.6': array([0]), 'hotbar.7': array([0]), 'hotbar.8': array([0]), 'hotbar.9': array([0]), 'camera': array([[ 0.        , -0.61539427]])}\n",
      "Action: {'attack': array([1]), 'back': array([0]), 'forward': array([0]), 'jump': array([0]), 'left': array([0]), 'right': array([0]), 'sneak': array([0]), 'sprint': array([0]), 'use': array([0]), 'drop': array([0]), 'inventory': array([0]), 'hotbar.1': array([0]), 'hotbar.2': array([0]), 'hotbar.3': array([0]), 'hotbar.4': array([0]), 'hotbar.5': array([0]), 'hotbar.6': array([0]), 'hotbar.7': array([0]), 'hotbar.8': array([0]), 'hotbar.9': array([0]), 'camera': array([[ 0.        , -0.61539427]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "obs = np.load(os.path.join(r\"F:\\16831_RL\\Proj\\MC_RL\\Model_Weights\\pre_log\\session_11_11_24_21-55\\obs_rew\", \"obs.npy\"), allow_pickle=True)\n",
    "print(obs.shape)\n",
    "print(obs[0]['pov'].shape)\n",
    "\n",
    "\n",
    "action = agent.get_action(obs[0])\n",
    "print(\"Action:\", action)\n",
    "action = agent.get_action(obs[1])\n",
    "print(\"Action:\", action)\n",
    "action = agent.get_action(obs[2])\n",
    "print(\"Action:\", action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'model', 'extra_args'])\n",
      "dict_keys(['net', 'pi_head_opts'])\n",
      "{'active_reward_monitors': {'craft_stats': {'args': {'collapse_var': True, 'items': ['planks', 'stick', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe', 'furnace', 'iron_ingot', 'iron_pickaxe', 'diamond_pickaxe', 'torch']}, 'weight': 0}, 'mine_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal_ore', 'stone', 'iron_ore', 'diamond_ore', 'obsidian']}, 'weight': 0}, 'order_invariant_curriculum': {'args': {'curriculum': {'coal': [5, 0.4], 'cobblestone': [11, 0.09090909090909091], 'crafting_table': [1, 1], 'diamond': [10000, 2.6666666666666665], 'diamond_pickaxe': [10000, 8], 'furnace': [1, 1], 'iron_ingot': [3, 1.3333333333333333], 'iron_ore': [3, 1.3333333333333333], 'iron_pickaxe': [1, 4], 'log': [8, 0.125], 'obsidian': [10000, 16], 'planks': [20, 0.05], 'stick': [16, 0.0625], 'stone_pickaxe': [1, 1], 'torch': [16, 0.125], 'wooden_pickaxe': [1, 1]}}, 'weight': 1}, 'pickup_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal', 'cobblestone', 'iron_ore', 'diamond']}, 'weight': 0}, 'variety': {'args': {'collapse_var': True, 'included_items': ['beef', 'chicken', 'leather', 'mutton', 'porkchop', 'bucket', 'milk_bucket', 'water_bucket', 'coal', 'crafting_table', 'furnace', 'diamond', 'gold_ingot', 'gold_ore', 'flint', 'iron_ingot', 'iron_ore', 'shears', 'string', 'cobblestone', 'log', 'planks', 'stick', 'wool', 'obsidian', 'paper', 'redstone', 'wheat', 'cooked_beef', 'cooked_chicken', 'cooked_mutton', 'cooked_porkchop', 'egg', 'feather', 'leather_boots', 'leather_chestplate', 'leather_helmet', 'leather_leggings', 'brick', 'brick_stairs', 'clay', 'clay_ball', 'flower_pot', 'terracotta', 'torch', 'diamond_axe', 'diamond_block', 'diamond_boots', 'diamond_chestplate', 'diamond_helmet', 'diamond_hoe', 'diamond_leggings', 'diamond_pickaxe', 'diamond_shovel', 'diamond_sword', 'dirt', 'golden_apple', 'golden_axe', 'golden_boots', 'golden_chestplate', 'golden_helmet', 'golden_hoe', 'golden_leggings', 'golden_pickaxe', 'golden_shovel', 'golden_sword', 'arrow', 'gravel', 'iron_axe', 'iron_boots', 'iron_chestplate', 'iron_helmet', 'iron_hoe', 'iron_leggings', 'iron_pickaxe', 'iron_shovel', 'iron_sword', 'shield', 'fermented_spider_eye', 'leaves', 'apple', 'bread', 'activator_rail', 'clock', 'compass', 'detector_rail', 'dropper', 'book', 'bookshelf', 'cake', 'filled_map', 'sugar_cane', 'sugar', 'bow', 'dispenser', 'fishing_rod', 'spider_eye', 'stone_axe', 'stone_hoe', 'stone_pickaxe', 'stone_shovel', 'stone_sword', 'boat', 'wooden_axe', 'wooden_hoe', 'wooden_pickaxe', 'wooden_shovel', 'wooden_sword', 'banner', 'bed', 'carpet', 'map', 'redstone_torch', 'wheat_seeds', 'flint_and_steel']}, 'weight': 0}}, 'attention_heads': 16, 'attention_mask_style': 'clipped_causal', 'attention_memory_size': 256, 'diff_mlp_embedding': False, 'hidsize': 2048, 'img_shape': [128, 128, 3], 'impala_chans': [16, 32, 32], 'impala_kwargs': {'post_pool_groups': 1}, 'impala_width': 8, 'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1}, 'n_recurrence_layers': 4, 'only_img_input': True, 'pointwise_ratio': 4, 'pointwise_use_activation': False, 'recurrence_is_residual': True, 'recurrence_type': 'transformer', 'timesteps': 128, 'use_pointwise_layer': True, 'use_pre_lstm_ln': False}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_path = r'F:\\16831_RL\\Proj\\MC_RL\\Model_Weights\\2x_pre\\2x.model'\n",
    "with open(model_path, 'rb') as f:\n",
    "    agent_parameters = pickle.load(f)\n",
    "\n",
    "print(agent_parameters.keys())\n",
    "print(agent_parameters['model']['args'].keys())\n",
    "print(agent_parameters['model']['args']['net']['args'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['net.img_process.cnn.stacks.0.firstconv.layer.weight', 'net.img_process.cnn.stacks.0.firstconv.layer.bias', 'net.img_process.cnn.stacks.0.n.weight', 'net.img_process.cnn.stacks.0.n.bias', 'net.img_process.cnn.stacks.0.blocks.0.conv0.norm.weight', 'net.img_process.cnn.stacks.0.blocks.0.conv0.norm.bias', 'net.img_process.cnn.stacks.0.blocks.0.conv0.layer.weight', 'net.img_process.cnn.stacks.0.blocks.0.conv1.norm.weight', 'net.img_process.cnn.stacks.0.blocks.0.conv1.norm.bias', 'net.img_process.cnn.stacks.0.blocks.0.conv1.layer.weight', 'net.img_process.cnn.stacks.0.blocks.1.conv0.norm.weight', 'net.img_process.cnn.stacks.0.blocks.1.conv0.norm.bias', 'net.img_process.cnn.stacks.0.blocks.1.conv0.layer.weight', 'net.img_process.cnn.stacks.0.blocks.1.conv1.norm.weight', 'net.img_process.cnn.stacks.0.blocks.1.conv1.norm.bias', 'net.img_process.cnn.stacks.0.blocks.1.conv1.layer.weight', 'net.img_process.cnn.stacks.1.firstconv.norm.weight', 'net.img_process.cnn.stacks.1.firstconv.norm.bias', 'net.img_process.cnn.stacks.1.firstconv.layer.weight', 'net.img_process.cnn.stacks.1.n.weight', 'net.img_process.cnn.stacks.1.n.bias', 'net.img_process.cnn.stacks.1.blocks.0.conv0.norm.weight', 'net.img_process.cnn.stacks.1.blocks.0.conv0.norm.bias', 'net.img_process.cnn.stacks.1.blocks.0.conv0.layer.weight', 'net.img_process.cnn.stacks.1.blocks.0.conv1.norm.weight', 'net.img_process.cnn.stacks.1.blocks.0.conv1.norm.bias', 'net.img_process.cnn.stacks.1.blocks.0.conv1.layer.weight', 'net.img_process.cnn.stacks.1.blocks.1.conv0.norm.weight', 'net.img_process.cnn.stacks.1.blocks.1.conv0.norm.bias', 'net.img_process.cnn.stacks.1.blocks.1.conv0.layer.weight', 'net.img_process.cnn.stacks.1.blocks.1.conv1.norm.weight', 'net.img_process.cnn.stacks.1.blocks.1.conv1.norm.bias', 'net.img_process.cnn.stacks.1.blocks.1.conv1.layer.weight', 'net.img_process.cnn.stacks.2.firstconv.norm.weight', 'net.img_process.cnn.stacks.2.firstconv.norm.bias', 'net.img_process.cnn.stacks.2.firstconv.layer.weight', 'net.img_process.cnn.stacks.2.n.weight', 'net.img_process.cnn.stacks.2.n.bias', 'net.img_process.cnn.stacks.2.blocks.0.conv0.norm.weight', 'net.img_process.cnn.stacks.2.blocks.0.conv0.norm.bias', 'net.img_process.cnn.stacks.2.blocks.0.conv0.layer.weight', 'net.img_process.cnn.stacks.2.blocks.0.conv1.norm.weight', 'net.img_process.cnn.stacks.2.blocks.0.conv1.norm.bias', 'net.img_process.cnn.stacks.2.blocks.0.conv1.layer.weight', 'net.img_process.cnn.stacks.2.blocks.1.conv0.norm.weight', 'net.img_process.cnn.stacks.2.blocks.1.conv0.norm.bias', 'net.img_process.cnn.stacks.2.blocks.1.conv0.layer.weight', 'net.img_process.cnn.stacks.2.blocks.1.conv1.norm.weight', 'net.img_process.cnn.stacks.2.blocks.1.conv1.norm.bias', 'net.img_process.cnn.stacks.2.blocks.1.conv1.layer.weight', 'net.img_process.cnn.dense.norm.weight', 'net.img_process.cnn.dense.norm.bias', 'net.img_process.cnn.dense.layer.weight', 'net.img_process.linear.norm.weight', 'net.img_process.linear.norm.bias', 'net.img_process.linear.layer.weight', 'net.recurrent_layer.blocks.0.mlp0.norm.weight', 'net.recurrent_layer.blocks.0.mlp0.norm.bias', 'net.recurrent_layer.blocks.0.mlp0.layer.weight', 'net.recurrent_layer.blocks.0.mlp1.layer.weight', 'net.recurrent_layer.blocks.0.mlp1.layer.bias', 'net.recurrent_layer.blocks.0.pre_r_ln.weight', 'net.recurrent_layer.blocks.0.pre_r_ln.bias', 'net.recurrent_layer.blocks.0.r.orc_block.b_nd', 'net.recurrent_layer.blocks.0.r.orc_block.q_layer.weight', 'net.recurrent_layer.blocks.0.r.orc_block.q_layer.bias', 'net.recurrent_layer.blocks.0.r.orc_block.k_layer.weight', 'net.recurrent_layer.blocks.0.r.orc_block.v_layer.weight', 'net.recurrent_layer.blocks.0.r.orc_block.proj_layer.weight', 'net.recurrent_layer.blocks.0.r.orc_block.proj_layer.bias', 'net.recurrent_layer.blocks.0.r.orc_block.r_layer.weight', 'net.recurrent_layer.blocks.0.r.orc_block.r_layer.bias', 'net.recurrent_layer.blocks.1.mlp0.norm.weight', 'net.recurrent_layer.blocks.1.mlp0.norm.bias', 'net.recurrent_layer.blocks.1.mlp0.layer.weight', 'net.recurrent_layer.blocks.1.mlp1.layer.weight', 'net.recurrent_layer.blocks.1.mlp1.layer.bias', 'net.recurrent_layer.blocks.1.pre_r_ln.weight', 'net.recurrent_layer.blocks.1.pre_r_ln.bias', 'net.recurrent_layer.blocks.1.r.orc_block.b_nd', 'net.recurrent_layer.blocks.1.r.orc_block.q_layer.weight', 'net.recurrent_layer.blocks.1.r.orc_block.q_layer.bias', 'net.recurrent_layer.blocks.1.r.orc_block.k_layer.weight', 'net.recurrent_layer.blocks.1.r.orc_block.v_layer.weight', 'net.recurrent_layer.blocks.1.r.orc_block.proj_layer.weight', 'net.recurrent_layer.blocks.1.r.orc_block.proj_layer.bias', 'net.recurrent_layer.blocks.1.r.orc_block.r_layer.weight', 'net.recurrent_layer.blocks.1.r.orc_block.r_layer.bias', 'net.recurrent_layer.blocks.2.mlp0.norm.weight', 'net.recurrent_layer.blocks.2.mlp0.norm.bias', 'net.recurrent_layer.blocks.2.mlp0.layer.weight', 'net.recurrent_layer.blocks.2.mlp1.layer.weight', 'net.recurrent_layer.blocks.2.mlp1.layer.bias', 'net.recurrent_layer.blocks.2.pre_r_ln.weight', 'net.recurrent_layer.blocks.2.pre_r_ln.bias', 'net.recurrent_layer.blocks.2.r.orc_block.b_nd', 'net.recurrent_layer.blocks.2.r.orc_block.q_layer.weight', 'net.recurrent_layer.blocks.2.r.orc_block.q_layer.bias', 'net.recurrent_layer.blocks.2.r.orc_block.k_layer.weight', 'net.recurrent_layer.blocks.2.r.orc_block.v_layer.weight', 'net.recurrent_layer.blocks.2.r.orc_block.proj_layer.weight', 'net.recurrent_layer.blocks.2.r.orc_block.proj_layer.bias', 'net.recurrent_layer.blocks.2.r.orc_block.r_layer.weight', 'net.recurrent_layer.blocks.2.r.orc_block.r_layer.bias', 'net.recurrent_layer.blocks.3.mlp0.norm.weight', 'net.recurrent_layer.blocks.3.mlp0.norm.bias', 'net.recurrent_layer.blocks.3.mlp0.layer.weight', 'net.recurrent_layer.blocks.3.mlp1.layer.weight', 'net.recurrent_layer.blocks.3.mlp1.layer.bias', 'net.recurrent_layer.blocks.3.pre_r_ln.weight', 'net.recurrent_layer.blocks.3.pre_r_ln.bias', 'net.recurrent_layer.blocks.3.r.orc_block.b_nd', 'net.recurrent_layer.blocks.3.r.orc_block.q_layer.weight', 'net.recurrent_layer.blocks.3.r.orc_block.q_layer.bias', 'net.recurrent_layer.blocks.3.r.orc_block.k_layer.weight', 'net.recurrent_layer.blocks.3.r.orc_block.v_layer.weight', 'net.recurrent_layer.blocks.3.r.orc_block.proj_layer.weight', 'net.recurrent_layer.blocks.3.r.orc_block.proj_layer.bias', 'net.recurrent_layer.blocks.3.r.orc_block.r_layer.weight', 'net.recurrent_layer.blocks.3.r.orc_block.r_layer.bias', 'net.lastlayer.norm.weight', 'net.lastlayer.norm.bias', 'net.lastlayer.layer.weight', 'net.final_ln.weight', 'net.final_ln.bias', 'value_head.linear.weight', 'value_head.linear.bias', 'value_head.normalizer.running_mean', 'value_head.normalizer.running_mean_sq', 'value_head.normalizer.debiasing_term', 'pi_head.buttons.linear_layer.weight', 'pi_head.buttons.linear_layer.bias', 'pi_head.camera.linear_layer.weight', 'pi_head.camera.linear_layer.bias', 'aux_value_head.linear.weight', 'aux_value_head.linear.bias', 'aux_value_head.normalizer.running_mean', 'aux_value_head.normalizer.running_mean_sq', 'aux_value_head.normalizer.debiasing_term'])\n",
      "139\n",
      "net.img_process.cnn.stacks.0.firstconv.layer.weight: torch.Size([128, 3, 3, 3])\n",
      "net.img_process.cnn.stacks.0.firstconv.layer.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.n.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.n.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv0.norm.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv0.norm.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv0.layer.weight: torch.Size([128, 128, 3, 3])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv1.norm.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv1.norm.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.0.conv1.layer.weight: torch.Size([128, 128, 3, 3])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv0.norm.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv0.norm.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv0.layer.weight: torch.Size([128, 128, 3, 3])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv1.norm.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv1.norm.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.0.blocks.1.conv1.layer.weight: torch.Size([128, 128, 3, 3])\n",
      "net.img_process.cnn.stacks.1.firstconv.norm.weight: torch.Size([128])\n",
      "net.img_process.cnn.stacks.1.firstconv.norm.bias: torch.Size([128])\n",
      "net.img_process.cnn.stacks.1.firstconv.layer.weight: torch.Size([256, 128, 3, 3])\n",
      "net.img_process.cnn.stacks.1.n.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.n.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv0.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv0.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv0.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv1.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv1.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.0.conv1.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv0.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv0.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv0.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv1.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv1.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.1.blocks.1.conv1.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.2.firstconv.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.firstconv.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.firstconv.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.2.n.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.n.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv0.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv0.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv0.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv1.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv1.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.0.conv1.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv0.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv0.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv0.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv1.norm.weight: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv1.norm.bias: torch.Size([256])\n",
      "net.img_process.cnn.stacks.2.blocks.1.conv1.layer.weight: torch.Size([256, 256, 3, 3])\n",
      "net.img_process.cnn.dense.norm.weight: torch.Size([65536])\n",
      "net.img_process.cnn.dense.norm.bias: torch.Size([65536])\n",
      "net.img_process.cnn.dense.layer.weight: torch.Size([256, 65536])\n",
      "net.img_process.linear.norm.weight: torch.Size([256])\n",
      "net.img_process.linear.norm.bias: torch.Size([256])\n",
      "net.img_process.linear.layer.weight: torch.Size([2048, 256])\n",
      "net.recurrent_layer.blocks.0.mlp0.norm.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.mlp0.norm.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.mlp0.layer.weight: torch.Size([8192, 2048])\n",
      "net.recurrent_layer.blocks.0.mlp1.layer.weight: torch.Size([2048, 8192])\n",
      "net.recurrent_layer.blocks.0.mlp1.layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.pre_r_ln.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.pre_r_ln.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.b_nd: torch.Size([10, 128])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.q_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.q_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.k_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.v_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.proj_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.proj_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.r_layer.weight: torch.Size([160, 2048])\n",
      "net.recurrent_layer.blocks.0.r.orc_block.r_layer.bias: torch.Size([160])\n",
      "net.recurrent_layer.blocks.1.mlp0.norm.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.mlp0.norm.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.mlp0.layer.weight: torch.Size([8192, 2048])\n",
      "net.recurrent_layer.blocks.1.mlp1.layer.weight: torch.Size([2048, 8192])\n",
      "net.recurrent_layer.blocks.1.mlp1.layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.pre_r_ln.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.pre_r_ln.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.b_nd: torch.Size([10, 128])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.q_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.q_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.k_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.v_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.proj_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.proj_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.r_layer.weight: torch.Size([160, 2048])\n",
      "net.recurrent_layer.blocks.1.r.orc_block.r_layer.bias: torch.Size([160])\n",
      "net.recurrent_layer.blocks.2.mlp0.norm.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.mlp0.norm.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.mlp0.layer.weight: torch.Size([8192, 2048])\n",
      "net.recurrent_layer.blocks.2.mlp1.layer.weight: torch.Size([2048, 8192])\n",
      "net.recurrent_layer.blocks.2.mlp1.layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.pre_r_ln.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.pre_r_ln.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.b_nd: torch.Size([10, 128])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.q_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.q_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.k_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.v_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.proj_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.proj_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.r_layer.weight: torch.Size([160, 2048])\n",
      "net.recurrent_layer.blocks.2.r.orc_block.r_layer.bias: torch.Size([160])\n",
      "net.recurrent_layer.blocks.3.mlp0.norm.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.mlp0.norm.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.mlp0.layer.weight: torch.Size([8192, 2048])\n",
      "net.recurrent_layer.blocks.3.mlp1.layer.weight: torch.Size([2048, 8192])\n",
      "net.recurrent_layer.blocks.3.mlp1.layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.pre_r_ln.weight: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.pre_r_ln.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.b_nd: torch.Size([10, 128])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.q_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.q_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.k_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.v_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.proj_layer.weight: torch.Size([2048, 2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.proj_layer.bias: torch.Size([2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.r_layer.weight: torch.Size([160, 2048])\n",
      "net.recurrent_layer.blocks.3.r.orc_block.r_layer.bias: torch.Size([160])\n",
      "net.lastlayer.norm.weight: torch.Size([2048])\n",
      "net.lastlayer.norm.bias: torch.Size([2048])\n",
      "net.lastlayer.layer.weight: torch.Size([2048, 2048])\n",
      "net.final_ln.weight: torch.Size([2048])\n",
      "net.final_ln.bias: torch.Size([2048])\n",
      "value_head.linear.weight: torch.Size([1, 2048])\n",
      "value_head.linear.bias: torch.Size([1])\n",
      "value_head.normalizer.running_mean: torch.Size([1])\n",
      "value_head.normalizer.running_mean_sq: torch.Size([1])\n",
      "value_head.normalizer.debiasing_term: torch.Size([])\n",
      "pi_head.buttons.linear_layer.weight: torch.Size([8641, 2048])\n",
      "pi_head.buttons.linear_layer.bias: torch.Size([8641])\n",
      "pi_head.camera.linear_layer.weight: torch.Size([121, 2048])\n",
      "pi_head.camera.linear_layer.bias: torch.Size([121])\n",
      "aux_value_head.linear.weight: torch.Size([1, 2048])\n",
      "aux_value_head.linear.bias: torch.Size([1])\n",
      "aux_value_head.normalizer.running_mean: torch.Size([1])\n",
      "aux_value_head.normalizer.running_mean_sq: torch.Size([1])\n",
      "aux_value_head.normalizer.debiasing_term: torch.Size([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheffield\\AppData\\Local\\Temp\\ipykernel_32880\\2951249431.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weights_path = r'F:\\16831_RL\\Proj\\MCRL-Proj\\Model_Weights\\2x_pre\\rl-from-house-2x.weights'\n",
    "weights = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "print(weights.keys())\n",
    "# print(weights)\n",
    "print(len(weights.keys()))\n",
    "# print(weights['net.img_process.cnn.stacks.0.firstconv.layer.weight'].shape)\n",
    "\n",
    "for key, value in weights.items():\n",
    "    print(f\"{key}: {value.shape if hasattr(value, 'shape') else 'Scalar'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheffield\\AppData\\Local\\Temp\\ipykernel_32880\\728237490.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "# Open the file containing MineRLAgent\n",
    "with open('agent.py', 'r') as file:\n",
    "    for i, line in enumerate(file, 1):\n",
    "        if 'lambda' in line:\n",
    "            print(f\"Line {i}: {line.strip()}\")\n",
    "\n",
    "weights_path = r'F:\\16831_RL\\Proj\\MCRL-Proj\\Model_Weights\\2x_pre\\rl-from-house-2x.weights'\n",
    "weights = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "for key, value in weights.items():\n",
    "    if callable(value):  # Check if any value is a function\n",
    "        print(f\"Key {key} contains a callable value: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shared_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save weights to a temporary file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m----> 5\u001b[0m     temp_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mshared_weights\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m     temp_file\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m      7\u001b[0m     agent\u001b[38;5;241m.\u001b[39mload_weights(temp_file\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shared_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "# Save weights to a temporary file\n",
    "with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n",
    "    temp_file.write(shared_weights[\"weights\"])\n",
    "    temp_file.flush()\n",
    "    agent.load_weights(temp_file.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
